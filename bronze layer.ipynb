{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f423e490-3e3e-45a8-8c71-ecdaa76035c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#so we created a ingestion config file for all the sources we have if a source name is change we dont have to change name manually eveywhere in our code and it help to orgnize eveything.\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# -------------------------------\n",
    "#  Load ingestion config\n",
    "# -------------------------------\n",
    "config_file_path = \"/Workspace/Users/forazure1995@gmail.com/ingestion_config.json\"\n",
    "\n",
    "with open(config_file_path, \"r\") as f:\n",
    "    INGESTION_CONFIG = json.load(f)\n",
    "\n",
    "# -------------------------------\n",
    "#  Base path for Bronze layer\n",
    "# -------------------------------\n",
    "BRONZE_BASE_PATH = \"abfss://bronze@fordatabricks111.dfs.core.windows.net/\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function to read any table\n",
    "# -------------------------------\n",
    "def read_table(table_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV from the config by logical table name\n",
    "    Returns a Spark DataFrame\n",
    "    \"\"\"\n",
    "    # Find the path from config\n",
    "    path = next(cfg[\"path\"] for cfg in INGESTION_CONFIG if cfg[\"table\"] == table_name)\n",
    "    return spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390450b6-2c9f-4959-a675-14cc7206aa09",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769538290462}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#now we can just use readtable function and ca read anytable easily\n",
    "df_prd = read_table(\"sales_details\")\n",
    "display(df_prd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0713d9e-b2b7-4019-b6b2-75ba42436c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ========================================\n",
    "-- CREATE BRONZE SCHEMA\n",
    "-- ========================================\n",
    "CREATE SCHEMA IF NOT EXISTS databrickswp.bronze\n",
    "MANAGED LOCATION 'abfss://bronze@fordatabricks111.dfs.core.windows.net/'\n",
    "COMMENT 'Bronze layer - Raw ingested data';\n",
    "\n",
    "-- ========================================\n",
    "-- CREATE SILVER SCHEMA\n",
    "-- ========================================\n",
    "CREATE SCHEMA IF NOT EXISTS databrickswp.silver\n",
    "MANAGED LOCATION 'abfss://silver@fordatabricks111.dfs.core.windows.net/'\n",
    "COMMENT 'Silver layer - Cleaned and validated data';\n",
    "\n",
    "-- ========================================\n",
    "-- CREATE GOLD SCHEMA\n",
    "-- ========================================\n",
    "CREATE SCHEMA IF NOT EXISTS databrickswp.gold\n",
    "MANAGED LOCATION 'abfss://gold@fordatabricks111.dfs.core.windows.net/'\n",
    "COMMENT 'Gold layer - Business-level aggregates and analytics';\n",
    "\n",
    "-- ========================================\n",
    "-- VERIFY SCHEMAS CREATED\n",
    "-- ========================================\n",
    "SHOW SCHEMAS IN databrickswp;\n",
    "\n",
    "-- Describe each schema to verify\n",
    "DESCRIBE SCHEMA EXTENDED databrickswp.bronze;\n",
    "DESCRIBE SCHEMA EXTENDED databrickswp.silver;\n",
    "DESCRIBE SCHEMA EXTENDED databrickswp.gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88531381-34e2-4455-82aa-56991b71fce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "write all csv files into bronze layer in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39a0a59-7bdd-46c8-8c3c-2eae531aefd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loop through ingestion config instead of hard-coded CSVs\n",
    "for cfg in INGESTION_CONFIG:\n",
    "    table_name = cfg[\"table\"]      # logical table name\n",
    "    source_path = cfg[\"path\"]      # CSV path from config\n",
    "\n",
    "    # Read CSV\n",
    "    df = spark.read.csv(source_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Save to Bronze folder with same name as table\n",
    "    target_path = f\"{bronze_path}{table_name}/\"  # use your existing bronze_path variable\n",
    "    df.write.mode(\"overwrite\").parquet(target_path)\n",
    "\n",
    "    print(f\"{table_name} saved to {target_path} as Parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067a0541-5fa3-4204-9082-74d696c37dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfff=read_table(\"sales_details\")\n",
    "display(dfff)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6164408823933715,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
